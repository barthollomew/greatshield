---
import Layout from '../layouts/Layout.astro';
import HeaderNav from '../components/HeaderNav.astro';
import Footer from '../components/Footer.astro';
import ModelTierCard from '../components/ModelTierCard';
import Card from '../components/Card';
import CodeBlock from '../components/CodeBlock';
---

<Layout title="AI Models - Greatshield">
  <HeaderNav />
  
  <main>
    <section class="container mx-auto px-4 py-16">
      <div class="max-w-6xl mx-auto">
        <h1 class="text-3xl md:text-5xl font-bold text-center mb-8 tracking-wide">
          Choose a Model That Fits Your Machine
        </h1>
        <p class="text-lg text-center text-foreground/80 mb-16 max-w-3xl mx-auto">
          Greatshield works with multiple AI models via Ollama. Pick the one that matches 
          your hardware capabilities and accuracy requirements.
        </p>

        <!-- Model tier cards -->
        <div class="grid lg:grid-cols-3 gap-8 mb-16">
          <ModelTierCard 
            client:load
            title="Low-End"
            subtitle="TinyLLaMA 1.1B (Q4_K_M)"
            ramUsage="~650MB"
            latency="100-300ms"
            features={[
              "Fastest inference on older hardware",
              "Minimal memory footprint", 
              "Basic content classification",
              "Good for simple filtering tasks"
            ]}
            modelName="tinyllama:1.1b-q4_k_m"
          />

          <ModelTierCard 
            client:load
            title="Mid-Range"
            subtitle="Phi-2 2.7B (Q4_K_M)"
            ramUsage="~850MB"
            latency="200-500ms"
            features={[
              "Excellent balance of speed & accuracy",
              "Strong reasoning capabilities",
              "Handles nuanced content well",
              "Recommended for most users"
            ]}
            modelName="phi:2.7b-q4_k_m"
            recommended={true}
          />

          <ModelTierCard 
            client:load
            title="High-End"
            subtitle="Mistral 7B Instruct (Q4_K_M)"
            ramUsage="~4.2GB"
            latency="500-1500ms"
            features={[
              "Best accuracy and reasoning",
              "Excellent context understanding",
              "Handles complex edge cases",
              "Ideal for large communities"
            ]}
            modelName="mistral:7b-instruct-q4_k_m"
          />
        </div>

        <!-- Custom model section -->
        <Card client:load>
          <h2 class="text-xl font-semibold mb-4 uppercase tracking-wide">
            Bring Your Own Model
          </h2>
          <p class="text-foreground/80 mb-4">
            Greatshield works with any Ollama-compatible model. Simply specify the model name 
            during setup or in your configuration.
          </p>
          <div class="grid md:grid-cols-2 gap-6">
            <div>
              <h3 class="text-sm font-semibold mb-2 uppercase tracking-wide text-accent">
                Popular Alternatives:
              </h3>
              <ul class="text-sm space-y-1 text-foreground/70">
                <li>• llama3.2:3b for balanced performance</li>
                <li>• codellama:7b for code-heavy servers</li>
                <li>• gemma2:2b for lightweight deployment</li>
                <li>• qwen2.5:7b for multilingual support</li>
              </ul>
            </div>
            <div>
              <h3 class="text-sm font-semibold mb-2 uppercase tracking-wide text-accent">
                Installation:
              </h3>
              <CodeBlock client:load>
{`# Pull your chosen model
ollama pull your-model:tag

# Configure Greatshield to use it
npx greatshield setup`}
              </CodeBlock>
            </div>
          </div>
        </Card>

        <!-- System requirements -->
        <div class="grid md:grid-cols-2 gap-8 mt-16">
          <Card client:load>
            <h2 class="text-xl font-semibold mb-4 uppercase tracking-wide">
              System Requirements
            </h2>
            <div class="space-y-4 text-sm">
              <div>
                <h3 class="font-semibold text-accent mb-2">Minimum:</h3>
                <ul class="space-y-1 text-foreground/80">
                  <li>• 4GB RAM (2GB available for AI)</li>
                  <li>• Dual-core CPU (2015 or newer)</li>
                  <li>• 10GB free disk space</li>
                  <li>• Node.js 20+ and npm</li>
                </ul>
              </div>
              
              <div>
                <h3 class="font-semibold text-accent mb-2">Recommended:</h3>
                <ul class="space-y-1 text-foreground/80">
                  <li>• 8GB RAM (4GB available for AI)</li>
                  <li>• Quad-core CPU with AVX2 support</li>
                  <li>• SSD storage for better model loading</li>
                  <li>• Stable internet for initial model download</li>
                </ul>
              </div>
            </div>
          </Card>

          <Card client:load>
            <h2 class="text-xl font-semibold mb-4 uppercase tracking-wide">
              Performance Tuning
            </h2>
            <div class="space-y-4 text-sm">
              <div>
                <h3 class="font-semibold text-accent mb-2">Memory Optimization:</h3>
                <ul class="space-y-1 text-foreground/80">
                  <li>• Close unnecessary applications</li>
                  <li>• Use Q4_K_M quantized models</li>
                  <li>• Consider CPU-only mode for stability</li>
                </ul>
              </div>
              
              <div>
                <h3 class="font-semibold text-accent mb-2">Speed Optimization:</h3>
                <ul class="space-y-1 text-foreground/80">
                  <li>• Enable GPU acceleration if available</li>
                  <li>• Use smaller models for high-traffic servers</li>
                  <li>• Adjust context length in Ollama settings</li>
                </ul>
              </div>
            </div>
          </Card>
        </div>
      </div>
    </section>
  </main>

  <Footer />
</Layout>